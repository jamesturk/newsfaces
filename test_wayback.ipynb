{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test wayback internet archive API and get url util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import sys\n",
    "import json\n",
    "import lxml.html\n",
    "import csv\n",
    "from wayback import WaybackClient, memento_url_data, WaybackSession\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "REQUEST_DELAY = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(url, session=None):\n",
    "    \"\"\"\n",
    "    Make a request to `url` and return the raw response.\n",
    "\n",
    "    This function ensure that the domain matches what is expected and that the rate limit\n",
    "    is obeyed.\n",
    "    \"\"\"\n",
    "    # check if URL starts with an allowed domain name\n",
    "    time.sleep(REQUEST_DELAY)\n",
    "    print(f\"Fetching {url}\")\n",
    "    if session:\n",
    "        resp = session.get(url)\n",
    "    else:\n",
    "        resp = requests.get(url)\n",
    "    return resp\n",
    "\n",
    "\n",
    "def make_link_absolute(rel_url, current_url):\n",
    "    \"\"\"\n",
    "    Given a relative URL like \"/abc/def\" or \"?page=2\"\n",
    "    and a complete URL like \"https://example.com/1/2/3\" this function will\n",
    "    combine the two yielding a URL like \"https://example.com/abc/def\"\n",
    "\n",
    "    Parameters:\n",
    "        * rel_url:      a URL or fragment\n",
    "        * current_url:  a complete URL used to make the request that contained a link to rel_url\n",
    "\n",
    "    Returns:\n",
    "        A full URL with protocol & domain that refers to rel_url.\n",
    "    \"\"\"\n",
    "    url = urlparse(current_url)\n",
    "    if rel_url.startswith(\"/\"):\n",
    "        return f\"{url.scheme}://{url.netloc}{rel_url}\"\n",
    "    elif rel_url.startswith(\"?\"):\n",
    "        return f\"{url.scheme}://{url.netloc}{url.path}{rel_url}\"\n",
    "    else:\n",
    "        return rel_url\n",
    "\n",
    "\n",
    "def parse_html(html):\n",
    "    \"\"\"\n",
    "    Parse HTML and return the root node.\n",
    "    \"\"\"\n",
    "    return lxml.html.fromstring(html)\n",
    "\n",
    "\n",
    "def page_grab(url, session=None):\n",
    "    response = make_request(url, session)\n",
    "    root = parse_html(response.text)\n",
    "    return root\n",
    "\n",
    "\n",
    "def create_csv(set1, title1, filename, set2=set(), title2=\"\"):\n",
    "    \"\"\"\n",
    "    turns list of articles and videos into a csv with\n",
    "    these values as respective columns.\n",
    "    args:\n",
    "    set1- items scraped (ex. article urls)\n",
    "    set2- second type of items scraped (ex. videos)\n",
    "    title1- column header for first type\n",
    "    title2- optional header for second type\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([title1, title2])\n",
    "        max_length = max(len(set1), len(set2))\n",
    "        for i in range(max_length):\n",
    "            row = [\n",
    "                list(set1)[i] if i < len(set1) else \"\",\n",
    "                list(set2)[i] if i < len(set2) else \"\",\n",
    "            ]\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def get_urls(url, selectors, session=None):\n",
    "    \"\"\"\n",
    "    This function takes a URLs and returns lists of URLs\n",
    "    for containing each article on that page.\n",
    "\n",
    "    Parameters:\n",
    "        * url:  a URL to a page of articles\n",
    "        * session: optional session object parameter\n",
    "        * selectors: a list of css selectors\n",
    "\n",
    "    Returns:\n",
    "        A list of article URLs on that page.\n",
    "    \"\"\"\n",
    "    response = page_grab(url, session)\n",
    "    urls = []\n",
    "    for selector in selectors:\n",
    "        container = response.cssselect(selector)\n",
    "        for j in container:\n",
    "            atr = j.cssselect(\"a\")\n",
    "            if atr and len(atr) > 0:\n",
    "                href = atr[0].get(\"href\")\n",
    "                if len(href) > 0:\n",
    "                    urls.append(make_link_absolute(href, \"https://web.archive.org/\"))\n",
    "    return urls\n",
    "\n",
    "\n",
    "def crawl_wayback(homepage, break_point, scraper_func, startdate, selectors=False):\n",
    "    \"\"\"\n",
    "    Take a politics homepage, or any source with a list of articles, finds all\n",
    "    copies in the archive, and scrapes all of the article links on that page.\n",
    "    args:\n",
    "        homepage- the homepage or politics page we are looking for across time\n",
    "        break_point- the approx. number of copies in the archive\n",
    "        scraper_func - the individual function built for scraping that page\n",
    "        startdate- the date you would like to begin scraping ('YYYYMMDD')\n",
    "        selectors- optional css selector parameter(to be used with scraper_func)\n",
    "    returns:\n",
    "        list of articles from startdate to present\n",
    "\n",
    "    \"\"\"\n",
    "    session = WaybackSession()\n",
    "    client = WaybackClient(session)\n",
    "    results = client.search(homepage, match_type=\"exact\", from_date=startdate)\n",
    "    crosstime_urls = list(itertools.islice(results, break_point))\n",
    "    post_date_articles = set()\n",
    "    for i in range(len(crosstime_urls)):\n",
    "        date = datetime.datetime.strptime(startdate, \"%Y%m%d\")\n",
    "        if crosstime_urls[i].timestamp.date() >= date.date():\n",
    "            if selectors:\n",
    "                articles = scraper_func(crosstime_urls[i].view_url, selectors, session)\n",
    "            else:\n",
    "                articles = scraper_func(crosstime_urls[i].view_url, session)\n",
    "            # converts archive links back to current article links\n",
    "            articles = [memento_url_data(item)[0] for item in articles]\n",
    "            post_date_articles.update(articles)\n",
    "    return post_date_articles\n",
    "\n",
    "def crawl_wayback_2(homepage, startdate, enddate, scraper_func, selectors=False, delta_hrs= 6):\n",
    "    #Create datetime - objects to crawl using wayback\n",
    "    year, month, day = startdate\n",
    "    current_date = datetime.datetime(year,month,day)\n",
    "    year, month, day = enddate\n",
    "    end_date = datetime.datetime(year,month,day)\n",
    "    post_date_articles = set()\n",
    "\n",
    "    session = WaybackSession()\n",
    "    client = WaybackClient(session)\n",
    "    last_url_visited = None\n",
    "\n",
    "    #Crawl internet archive once every delta_hrs from startdate until enddate\n",
    "    while current_date != end_date:\n",
    "        results = client.search(homepage, match_type=\"exact\", from_date=current_date)\n",
    "        record = next(results)\n",
    "        url = record.view_url\n",
    "        #To avoid fetching urls multiple times, check if there are no updates in\n",
    "        #the delta_hrs period\n",
    "        if last_url_visited != url:\n",
    "            articles = scraper_func(url,selectors,session)\n",
    "            articles = [memento_url_data(item)[0] for item in articles]\n",
    "            post_date_articles.update(articles)\n",
    "\n",
    "        last_url_visited = url\n",
    "        current_date += datetime.timedelta(hours = delta_hrs)\n",
    "    return post_date_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://web.archive.org/web/20220101012450/https://www.washingtontimes.com/news/politics/?page=1\n",
      "Fetching https://web.archive.org/web/20220103060359/https://www.washingtontimes.com/news/politics/?page=1\n",
      "Fetching https://web.archive.org/web/20220104062753/https://www.washingtontimes.com/news/politics/?page=1\n",
      "Fetching https://web.archive.org/web/20220105064436/https://www.washingtontimes.com/news/politics/?page=1\n",
      "Fetching https://web.archive.org/web/20220107000917/https://www.washingtontimes.com/news/politics/?page=1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jpmartinezclaeys/Desktop/U Chicago/James Turk - RA/newsfaces/test_wayback.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/test_wayback.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m articles\u001b[39m=\u001b[39mcrawl_wayback_2(\u001b[39m\"\u001b[39;49m\u001b[39mhttps://www.washingtontimes.com/news/politics/?page=1\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39m2022\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m], [\u001b[39m2022\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m10\u001b[39;49m], get_urls, [\u001b[39m'\u001b[39;49m\u001b[39marticle\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[1;32m/Users/jpmartinezclaeys/Desktop/U Chicago/James Turk - RA/newsfaces/test_wayback.ipynb Cell 4\u001b[0m in \u001b[0;36mcrawl_wayback_2\u001b[0;34m(homepage, startdate, enddate, scraper_func, selectors, delta_hrs)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/test_wayback.ipynb#X30sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39mwhile\u001b[39;00m current_date \u001b[39m!=\u001b[39m end_date:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/test_wayback.ipynb#X30sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m     results \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39msearch(homepage, match_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexact\u001b[39m\u001b[39m\"\u001b[39m, from_date\u001b[39m=\u001b[39mcurrent_date)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/test_wayback.ipynb#X30sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m     record \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(results)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/test_wayback.ipynb#X30sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m     url \u001b[39m=\u001b[39m record\u001b[39m.\u001b[39mview_url\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/test_wayback.ipynb#X30sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     \u001b[39m#To avoid fetching urls multiple times, check if there are no updates in\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/test_wayback.ipynb#X30sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m     \u001b[39m#the delta_hrs period\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/wayback/_client.py:664\u001b[0m, in \u001b[0;36mWaybackClient.search\u001b[0;34m(self, url, match_type, limit, offset, fast_latest, from_date, to_date, filter_field, collapse, resolve_revisits, skip_malformed_results, matchType, fastLatest, resolveRevisits)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[39mwhile\u001b[39;00m next_query:\n\u001b[1;32m    663\u001b[0m     sent_query, next_query \u001b[39m=\u001b[39m next_query, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     \u001b[39mwith\u001b[39;00m _utils\u001b[39m.\u001b[39mrate_limited(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39msearch_calls_per_second,\n\u001b[1;32m    665\u001b[0m                              group\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msearch\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    666\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mrequest(\u001b[39m'\u001b[39m\u001b[39mGET\u001b[39m\u001b[39m'\u001b[39m, CDX_SEARCH_URL,\n\u001b[1;32m    667\u001b[0m                                         params\u001b[39m=\u001b[39msent_query)\n\u001b[1;32m    668\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    669\u001b[0m             \u001b[39m# Read/cache the response and close straightaway. If we need\u001b[39;00m\n\u001b[1;32m    670\u001b[0m             \u001b[39m# to raise for status, we want to pre-emptively close the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[39m# close the connection so it doesn't leak when we move onto\u001b[39;00m\n\u001b[1;32m    674\u001b[0m             \u001b[39m# the next of results or when this iterator ends.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/wayback/_utils.py:229\u001b[0m, in \u001b[0;36mrate_limited\u001b[0;34m(calls_per_second, group)\u001b[0m\n\u001b[1;32m    227\u001b[0m     current_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    228\u001b[0m     \u001b[39mif\u001b[39;00m current_time \u001b[39m-\u001b[39m last_call \u001b[39m<\u001b[39m minimum_wait:\n\u001b[0;32m--> 229\u001b[0m         time\u001b[39m.\u001b[39;49msleep(minimum_wait \u001b[39m-\u001b[39;49m (current_time \u001b[39m-\u001b[39;49m last_call))\n\u001b[1;32m    230\u001b[0m     _last_call_by_group[group] \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    231\u001b[0m \u001b[39myield\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "articles=crawl_wayback_2(\"https://www.washingtontimes.com/news/politics/?page=1\", [2022,1,1], [2022,1,10], get_urls, ['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://web.archive.org/web/20230101010133/https://www.nytimes.com/section/politics'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = WaybackSession()\n",
    "client = WaybackClient(session)\n",
    "results = client.search(\"https://www.nytimes.com/section/politics\", match_type=\"exact\", from_date=\"20230101\")\n",
    "record = next(results)\n",
    "record.view_url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST get urls  and wayback with scrapers written by JP\n",
    "\n",
    "#NYT\n",
    "nyt_test = get_urls(\"https://web.archive.org/web/20230716222629/https://www.nytimes.com/section/politics\",[\"article.css-1l4spti\"])\n",
    "\n",
    "\n",
    "#Test to see how back we can go with \"articles\" as css selector\n",
    "nyt_crawler_test_23 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2023,1,1], [2023,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_23)\n",
    "nyt_crawler_test_22 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2022,1,1], [2022,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_22)\n",
    "nyt_crawler_test_21 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2021,1,1], [2021,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_21)\n",
    "nyt_crawler_test_20 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2020,1,1], [2020,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_20)\n",
    "nyt_crawler_test_19 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2019,1,1], [2019,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_19)\n",
    "nyt_crawler_test_18 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2018,1,1], [2018,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_18)\n",
    "nyt_crawler_test_17 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2017,1,1], [2017,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_17)\n",
    "nyt_crawler_test_16 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2016,1,1], [2016,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_depth_wayback_crawler(homepage,  scraper_func, selectors=False, delta= False, min_year=2015, max_year=2023):\n",
    "    years = [*range(min_year,max_year+1,1)]\n",
    "    for year in reversed(years):\n",
    "        print(\"Testing year\",year)\n",
    "        startdate = [year,1,1]\n",
    "        enddate = [year,1,3]\n",
    "        year_test = crawl_wayback_2(homepage, startdate, enddate, scraper_func, selectors, delta)\n",
    "        if len(year_test) == 0:\n",
    "            return print(\"No results using this CSS selector in year\",year)\n",
    "    \n",
    "    return print(\"Selectors work for period between\",min_year, \"and\", max_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing year 2023\n",
      "Fetching https://web.archive.org/web/20230101010133/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20230102001011/http://nytimes.com/section/politics\n",
      "Testing year 2022\n",
      "Fetching https://web.archive.org/web/20220101040044/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20220102060757/http://nytimes.com/section/politics\n",
      "Testing year 2021\n",
      "Fetching https://web.archive.org/web/20210101010533/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20210102070144/https://www.nytimes.com/section/politics\n",
      "Testing year 2020\n",
      "Fetching https://web.archive.org/web/20200101005908/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20200102025944/https://www.nytimes.com/section/politics\n",
      "Testing year 2019\n",
      "Fetching https://web.archive.org/web/20190101041136/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20190102013343/https://www.nytimes.com/section/politics\n",
      "Testing year 2018\n",
      "Fetching https://web.archive.org/web/20180101144534/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20180102153950/https://www.nytimes.com/section/politics\n",
      "Testing year 2017\n",
      "Fetching https://web.archive.org/web/20170614233714/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20170614233714/https://www.nytimes.com/section/politics\n",
      "Testing year 2016\n",
      "Fetching https://web.archive.org/web/20170614233714/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20170614233714/https://www.nytimes.com/section/politics\n",
      "Testing year 2015\n",
      "Fetching https://web.archive.org/web/20170614233714/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20170614233714/https://www.nytimes.com/section/politics\n",
      "Selectors work for period between 2015 and 2023\n"
     ]
    }
   ],
   "source": [
    "test_depth_wayback_crawler(\"https://www.nytimes.com/section/politics\", get_urls, ['article'],1,2015,2023)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
