{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test wayback internet archive API and get url util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import sys\n",
    "import json\n",
    "import lxml.html\n",
    "import csv\n",
    "from wayback import WaybackClient, memento_url_data, WaybackSession\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "REQUEST_DELAY = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(url, session=None):\n",
    "    \"\"\"\n",
    "    Make a request to `url` and return the raw response.\n",
    "\n",
    "    This function ensure that the domain matches what is expected and that the rate limit\n",
    "    is obeyed.\n",
    "    \"\"\"\n",
    "    # check if URL starts with an allowed domain name\n",
    "    time.sleep(REQUEST_DELAY)\n",
    "    print(f\"Fetching {url}\")\n",
    "    if session:\n",
    "        resp = session.get(url)\n",
    "    else:\n",
    "        resp = requests.get(url)\n",
    "    return resp\n",
    "\n",
    "\n",
    "def make_link_absolute(rel_url, current_url):\n",
    "    \"\"\"\n",
    "    Given a relative URL like \"/abc/def\" or \"?page=2\"\n",
    "    and a complete URL like \"https://example.com/1/2/3\" this function will\n",
    "    combine the two yielding a URL like \"https://example.com/abc/def\"\n",
    "\n",
    "    Parameters:\n",
    "        * rel_url:      a URL or fragment\n",
    "        * current_url:  a complete URL used to make the request that contained a link to rel_url\n",
    "\n",
    "    Returns:\n",
    "        A full URL with protocol & domain that refers to rel_url.\n",
    "    \"\"\"\n",
    "    url = urlparse(current_url)\n",
    "    if rel_url.startswith(\"/\"):\n",
    "        return f\"{url.scheme}://{url.netloc}{rel_url}\"\n",
    "    elif rel_url.startswith(\"?\"):\n",
    "        return f\"{url.scheme}://{url.netloc}{url.path}{rel_url}\"\n",
    "    else:\n",
    "        return rel_url\n",
    "\n",
    "\n",
    "def parse_html(html):\n",
    "    \"\"\"\n",
    "    Parse HTML and return the root node.\n",
    "    \"\"\"\n",
    "    return lxml.html.fromstring(html)\n",
    "\n",
    "\n",
    "def page_grab(url, session=None):\n",
    "    response = make_request(url, session)\n",
    "    root = parse_html(response.text)\n",
    "    return root\n",
    "\n",
    "\n",
    "def create_csv(set1, title1, filename, set2=set(), title2=\"\"):\n",
    "    \"\"\"\n",
    "    turns list of articles and videos into a csv with\n",
    "    these values as respective columns.\n",
    "    args:\n",
    "    set1- items scraped (ex. article urls)\n",
    "    set2- second type of items scraped (ex. videos)\n",
    "    title1- column header for first type\n",
    "    title2- optional header for second type\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([title1, title2])\n",
    "        max_length = max(len(set1), len(set2))\n",
    "        for i in range(max_length):\n",
    "            row = [\n",
    "                list(set1)[i] if i < len(set1) else \"\",\n",
    "                list(set2)[i] if i < len(set2) else \"\",\n",
    "            ]\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def get_urls(url, selectors, session=None):\n",
    "    \"\"\"\n",
    "    This function takes a URLs and returns lists of URLs\n",
    "    for containing each article on that page.\n",
    "\n",
    "    Parameters:\n",
    "        * url:  a URL to a page of articles\n",
    "        * session: optional session object parameter\n",
    "        * selectors: a list of css selectors\n",
    "\n",
    "    Returns:\n",
    "        A list of article URLs on that page.\n",
    "    \"\"\"\n",
    "    response = page_grab(url, session)\n",
    "    urls = []\n",
    "    for selector in selectors:\n",
    "        container = response.cssselect(selector)\n",
    "        for j in container:\n",
    "            atr = j.cssselect(\"a\")\n",
    "            if atr and len(atr) > 0:\n",
    "                href = atr[0].get(\"href\")\n",
    "                if len(href) > 0:\n",
    "                    urls.append(make_link_absolute(href, \"https://web.archive.org/\"))\n",
    "    return urls\n",
    "\n",
    "\n",
    "def crawl_wayback(homepage, break_point, scraper_func, startdate, selectors=False):\n",
    "    \"\"\"\n",
    "    Take a politics homepage, or any source with a list of articles, finds all\n",
    "    copies in the archive, and scrapes all of the article links on that page.\n",
    "    args:\n",
    "        homepage- the homepage or politics page we are looking for across time\n",
    "        break_point- the approx. number of copies in the archive\n",
    "        scraper_func - the individual function built for scraping that page\n",
    "        startdate- the date you would like to begin scraping ('YYYYMMDD')\n",
    "        selectors- optional css selector parameter(to be used with scraper_func)\n",
    "    returns:\n",
    "        list of articles from startdate to present\n",
    "\n",
    "    \"\"\"\n",
    "    session = WaybackSession()\n",
    "    client = WaybackClient(session)\n",
    "    results = client.search(homepage, match_type=\"exact\", from_date=startdate)\n",
    "    crosstime_urls = list(itertools.islice(results, break_point))\n",
    "    post_date_articles = set()\n",
    "    for i in range(len(crosstime_urls)):\n",
    "        date = datetime.datetime.strptime(startdate, \"%Y%m%d\")\n",
    "        if crosstime_urls[i].timestamp.date() >= date.date():\n",
    "            if selectors:\n",
    "                articles = scraper_func(crosstime_urls[i].view_url, selectors, session)\n",
    "            else:\n",
    "                articles = scraper_func(crosstime_urls[i].view_url, session)\n",
    "            # converts archive links back to current article links\n",
    "            articles = [memento_url_data(item)[0] for item in articles]\n",
    "            post_date_articles.update(articles)\n",
    "    return post_date_articles\n",
    "\n",
    "def crawl_wayback_2(homepage, startdate, enddate, scraper_func, selectors=False, delta_hrs= 6):\n",
    "    #Create datetime - objects to crawl using wayback\n",
    "    year, month, day = startdate\n",
    "    current_date = datetime.datetime(year,month,day)\n",
    "    year, month, day = enddate\n",
    "    end_date = datetime.datetime(year,month,day)\n",
    "    post_date_articles = set()\n",
    "\n",
    "    session = WaybackSession()\n",
    "    client = WaybackClient(session)\n",
    "\n",
    "    #Crawl interner archive once per day from startdate until enddate\n",
    "    while current_date != end_date:\n",
    "        \n",
    "        results = client.search(homepage, match_type=\"exact\", from_date=current_date)\n",
    "        record = next(results)\n",
    "        url = record.view_url\n",
    "        articles = scraper_func(url,selectors,session)\n",
    "        articles = [memento_url_data(item)[0] for item in articles]\n",
    "        post_date_articles.update(articles)\n",
    "        current_date += datetime.timedelta(days = delta_hrs)\n",
    "    return post_date_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=crawl_wayback_2(\"https://www.washingtontimes.com/news/politics/?page=1\", [2022,1,1], [2022,12,31], get_urls, ['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://web.archive.org/web/20230101010133/https://www.nytimes.com/section/politics'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = WaybackSession()\n",
    "client = WaybackClient(session)\n",
    "results = client.search(\"https://www.nytimes.com/section/politics\", match_type=\"exact\", from_date=\"20230101\")\n",
    "record = next(results)\n",
    "record.view_url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST get urls  and wayback with scrapers written by JP\n",
    "\n",
    "#NYT\n",
    "nyt_test = get_urls(\"https://web.archive.org/web/20230716222629/https://www.nytimes.com/section/politics\",[\"article.css-1l4spti\"])\n",
    "\n",
    "\n",
    "#Test to see how back we can go with \"articles\" as css selector\n",
    "nyt_crawler_test_23 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2023,1,1], [2023,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_23)\n",
    "nyt_crawler_test_22 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2022,1,1], [2022,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_22)\n",
    "nyt_crawler_test_21 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2021,1,1], [2021,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_21)\n",
    "nyt_crawler_test_20 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2020,1,1], [2020,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_20)\n",
    "nyt_crawler_test_19 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2019,1,1], [2019,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_19)\n",
    "nyt_crawler_test_18 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2018,1,1], [2018,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_18)\n",
    "nyt_crawler_test_17 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2017,1,1], [2017,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_17)\n",
    "nyt_crawler_test_16 = crawl_wayback_2(\"https://www.nytimes.com/section/politics\", [2016,1,1], [2016,1,3], get_urls, ['article'])\n",
    "print(nyt_crawler_test_16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_depth_wayback_crawler(homepage,  scraper_func, selectors=False, delta= False, min_year=2015, max_year=2023):\n",
    "    years = [*range(min_year,max_year+1,1)]\n",
    "    for year in reversed(years):\n",
    "        print(\"Testing year\",year)\n",
    "        startdate = [year,1,1]\n",
    "        enddate = [year,1,3]\n",
    "        year_test = crawl_wayback_2(homepage, startdate, enddate, scraper_func, selectors, delta)\n",
    "        if len(year_test) == 0:\n",
    "            return print(\"No results using this CSS selector in year\",year)\n",
    "    \n",
    "    return print(\"Selectors work for period between\",min_year, \"and\", max_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing year 2023\n",
      "Fetching https://web.archive.org/web/20230101010133/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20230102001011/http://nytimes.com/section/politics\n",
      "Testing year 2022\n",
      "Fetching https://web.archive.org/web/20220101040044/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20220102060757/http://nytimes.com/section/politics\n",
      "Testing year 2021\n",
      "Fetching https://web.archive.org/web/20210101010533/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20210102070144/https://www.nytimes.com/section/politics\n",
      "Testing year 2020\n",
      "Fetching https://web.archive.org/web/20200101005908/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20200102025944/https://www.nytimes.com/section/politics\n",
      "Testing year 2019\n",
      "Fetching https://web.archive.org/web/20190101041136/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20190102013343/https://www.nytimes.com/section/politics\n",
      "Testing year 2018\n",
      "Fetching https://web.archive.org/web/20180101144534/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20180102153950/https://www.nytimes.com/section/politics\n",
      "Testing year 2017\n",
      "Fetching https://web.archive.org/web/20170614233714/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20170614233714/https://www.nytimes.com/section/politics\n",
      "Testing year 2016\n",
      "Fetching https://web.archive.org/web/20170614233714/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20170614233714/https://www.nytimes.com/section/politics\n",
      "Testing year 2015\n",
      "Fetching https://web.archive.org/web/20170614233714/https://www.nytimes.com/section/politics\n",
      "Fetching https://web.archive.org/web/20170614233714/https://www.nytimes.com/section/politics\n",
      "Selectors work for period between 2015 and 2023\n"
     ]
    }
   ],
   "source": [
    "test_depth_wayback_crawler(\"https://www.nytimes.com/section/politics\", get_urls, ['article'],1,2015,2023)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
