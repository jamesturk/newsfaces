{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain CNN URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import time\n",
    "import requests\n",
    "import urllib\n",
    "import lxml.html\n",
    "from bs4 import BeautifulSoup\n",
    "import newspaper\n",
    "import trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Div_list [<Element div at 0x7f84e9239180>]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "\n",
    "def obtain_page_urls(from_art=\"0\",page=\"1\"):\n",
    "    url = \"https://www.cnn.com/search?q=politics&from={}&size=10&page={}&sort=newest&types=all&section=politics\".format(from_art,page)\n",
    "    resp = requests.get(url)\n",
    "    root = lxml.html.fromstring(resp.text)\n",
    "    div_list = root.cssselect(\"div.search__results-list\")\n",
    "    print(\"Div_list\",div_list)\n",
    "    article_links = div_list[0].cssselect(\"a\")\n",
    "    article_list = []\n",
    "    a_elements = root.cssselect(\"a.container__link\")\n",
    "    print(a_elements)\n",
    "    #for link in article_links:\n",
    "    #    article_list.append(link.get(\"href\"))\n",
    "\n",
    "obtain_page_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_cnn_urls(max_pages_to_crawl = 50):\n",
    "    cnn_links = []\n",
    "    url = \"https://www.cnn.com/search?q=politics&from=0&size=10&page=1&sort=newest&types=all&section=politics\"\n",
    "    resp = requests.get(url)\n",
    "    root = lxml.html.fromstring(resp.text)\n",
    "    ##\n",
    "\n",
    "    max_pages_text = root.cssselect(\"div.search__results-count\")[0]\n",
    "    ##Write lines to obtain number of pages \n",
    "    max_pages = \n",
    "    if max_pages < max_pages_to_crawl:\n",
    "        pages_to_crawl = max_pages\n",
    "    else:\n",
    "        pages_to_crawl = max_pages_to_crawl\n",
    "\n",
    "    #Crawl and retrieve Urls using helper function\n",
    "    from_art = 0\n",
    "    page = 1\n",
    "\n",
    "    for page in range(0,max_pages):\n",
    "        cnn_links.extend(obtain_page_urls(from_art,page))\n",
    "        from_art += 10\n",
    "        page += 1\n",
    "\n",
    "    cnn_links = set(cnn_links)\n",
    "    return cnn_links"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
