{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain NYT URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import urllib\n",
    "import lxml.html\n",
    "from bs4 import BeautifulSoup\n",
    "import newspaper\n",
    "import trafilatura\n",
    "import calendar\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_api_key = \"AUtPPNuICzon3X9uEaX2k4Dgs0YGBA25\"\n",
    "START_DATE = datetime.datetime(2015,1,1,0,0, tzinfo=pytz.timezone(\"utc\"))\n",
    "END_DATE = datetime.datetime.now(pytz.timezone(\"utc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Standard values\"\n",
    "POLITICS_SECTION = \"politics\"\n",
    "QUERY_LIST = [\"Donald Trump\", \"Joe Biden\"]\n",
    "FILTER_LIST = [\"headline\", \"lead_paragraph\"]\n",
    "nyt_api_key = \"AUtPPNuICzon3X9uEaX2k4Dgs0YGBA25\" ##Move to config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.nytimes.com/svc/search/v2/articlesearch.json?fq=news_desk=politicsAND section=Washington&begin_date=20150101&end_date=20230731&page=0&sort=oldest&api-key=AUtPPNuICzon3X9uEaX2k4Dgs0YGBA25'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_url(tags=None,news_desk=POLITICS_SECTION, section = \"Washington\", filters=FILTER_LIST, begin_date=START_DATE,\n",
    " end_date=END_DATE, page=\"0\"):\n",
    "    \"\"\"\n",
    "    Create request url for API based on query search parameters passed to the\n",
    "        function.\n",
    "    \n",
    "    Inputs:\n",
    "        tags (lst): list of tags (strings) to look for. The tags to filter for\n",
    "            are looked in the filters defined in the \"filters\" parameter.\n",
    "        filters (lst): list of filters where to look tags. They can be \"headline\",\n",
    "            \"lead_paragraph\" and/or \"body\"\n",
    "        begin_date (str): 8 digits (YYYYMMDD) string that specify the begin date\n",
    "            or from when to start looking for articles.\n",
    "        end_date (str): 8 digits (YYYYMMDD) string that specify the end date or\n",
    "            until when to stop looking for articles.\n",
    "        page (str): number of page string that states where to look for articles.\n",
    "\n",
    "    Return (str): URL string with query to send request to NYT Article Search \n",
    "        API\n",
    "    \"\"\"\n",
    "    begin_date = begin_date.strftime(\"%Y%m%d\")\n",
    "    end_date = end_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "    endpoint = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?\" \n",
    "    \n",
    "\n",
    "    #headline = \"(headline=\" + \" OR \".join(filters_copy) + \")\"\n",
    "    #news_desk = \"(news_desk)\" + news_desk\n",
    "\n",
    "\n",
    "    if tags:\n",
    "        tags_copy = tags[:]\n",
    "        filters_copy = filters[:]\n",
    "        for i,tag in enumerate(tags_copy):\n",
    "            tags_copy[i] = \"\\\"\" + tag + \"\\\"\"\n",
    "        for i, fil in enumerate(filters_copy):\n",
    "            filters_copy[i] = fil + \":(\" + \" OR \".join(tags_copy) + \")\"\n",
    "        fq = \"fq=\" + \" OR \".join(filters_copy) + \" AND news_desk=\" + news_desk + \" AND section=\" + section\n",
    "    else:\n",
    "        fq = \"fq=\" + \"news_desk=\" + news_desk + \"AND section=\" + section\n",
    "    url = endpoint + fq + \"&begin_date=\" + begin_date + \"&end_date=\" + end_date +\\\n",
    "            \"&page=\" + page + \"&sort=oldest\" + \"&api-key=\" + nyt_api_key\n",
    "\n",
    "    return url\n",
    "\n",
    "create_url()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(tags=None,news_desk=POLITICS_SECTION, section = \"Washington\", filters=FILTER_LIST, begin_date=START_DATE,\n",
    " end_date=END_DATE, page=\"0\"):\n",
    "    \"\"\"\n",
    "    Make a GET request to the NYT Article Search API with a request delay of 6\n",
    "        seconds to avoid reaching request limit of 60 requests per minute.\n",
    "\n",
    "    Inputs:\n",
    "        tags (lst): list of tags (strings) to look for. The tags to filter for\n",
    "            are looked in the body, headline and byline of the articles.\n",
    "        filters (lst): list of filters where to look tags. They can be \"headline\",\n",
    "            \"lead_paragraph\" and/or \"body\"\n",
    "        begin_date (str): 8 digits (YYYYMMDD) string that specify the begin date\n",
    "            or from when to start looking for articles.\n",
    "        end_date (str): 8 digits (YYYYMMDD) string that specify the end date or\n",
    "            until when to stop looking for articles.\n",
    "        page (str): number of page string that states where to look for articles.\n",
    "    \n",
    "    Return (Response): API request response with specified query parameters\n",
    "    \"\"\"\n",
    "\n",
    "    url = create_url(tags, news_desk, section, filters, begin_date, end_date, page)\n",
    "    time.sleep(1)\n",
    "    resp = requests.get(url)\n",
    "    resp_json = json.loads(resp.text)\n",
    "\n",
    "    return resp_json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hits': 129724, 'offset': 0, 'time': 53}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_json = make_request()\n",
    "resp_json[\"response\"][\"meta\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jpmartinezclaeys/Desktop/U Chicago/James Turk - RA/newsfaces/url_scrapers/nyt_url_scraper.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/url_scrapers/nyt_url_scraper.ipynb#X16sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/url_scrapers/nyt_url_scraper.ipynb#X16sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m page_responses\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/url_scrapers/nyt_url_scraper.ipynb#X16sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m test \u001b[39m=\u001b[39m crawl(begin_date\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m20230101\u001b[39;49m\u001b[39m\"\u001b[39;49m, end_date\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m20230131\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/jpmartinezclaeys/Desktop/U Chicago/James Turk - RA/newsfaces/url_scrapers/nyt_url_scraper.ipynb Cell 8\u001b[0m in \u001b[0;36mcrawl\u001b[0;34m(tags, news_desk, section, filters, begin_date, end_date)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/url_scrapers/nyt_url_scraper.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m max_pages \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(hits \u001b[39m/\u001b[39m \u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/url_scrapers/nyt_url_scraper.ipynb#X16sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mfor\u001b[39;00m page_n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_pages \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/url_scrapers/nyt_url_scraper.ipynb#X16sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39m#NYT has a limit of 5 requests per minute\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/url_scrapers/nyt_url_scraper.ipynb#X16sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m12\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/url_scrapers/nyt_url_scraper.ipynb#X16sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     resp \u001b[39m=\u001b[39m make_request(page \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(page_n))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jpmartinezclaeys/Desktop/U%20Chicago/James%20Turk%20-%20RA/newsfaces/url_scrapers/nyt_url_scraper.ipynb#X16sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     status \u001b[39m=\u001b[39m resp\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def make_link_absolute(rel_url, current_url):\n",
    "    \"\"\"\n",
    "    Given a relative URL like \"/abc/def\" or \"?page=2\"\n",
    "    and a complete URL like \"https://example.com/1/2/3\" this function will\n",
    "    combine the two yielding a URL like \"https://example.com/abc/def\"\n",
    "\n",
    "    Parameters:\n",
    "        * rel_url:      a URL or fragment\n",
    "        * current_url:  a complete URL used to make the request that contained a link to rel_url\n",
    "\n",
    "    Returns:\n",
    "        A full URL with protocol & domain that refers to rel_url.\n",
    "    \"\"\"\n",
    "    url = urlparse(current_url)\n",
    "    if rel_url.startswith(\"/\"):\n",
    "        return f\"{url.scheme}://{url.netloc}{rel_url}\"\n",
    "    elif rel_url.startswith(\"?\"):\n",
    "        return f\"{url.scheme}://{url.netloc}{url.path}{rel_url}\"\n",
    "    else:\n",
    "        return rel_url\n",
    "\n",
    "def obtain_articles_info(json_resp):\n",
    "    articles = json_resp[\"response\"][\"docs\"]\n",
    "    for article in articles:\n",
    "        multimedia = article[\"multimedia\"]\n",
    "        for item in multimedia:\n",
    "            url = \"/\" + item[\"url\"]\n",
    "            item[\"url\"] = make_link_absolute(url,\"https://www.nytimes.com/images/\")\n",
    "    return articles\n",
    "\n",
    "def crawl(tags=None,news_desk=POLITICS_SECTION, section = \"Washington\", \n",
    "        filters=FILTER_LIST, begin_date=\"20210101\", end_date=\"20230101\"):\n",
    "        \"\"\"\n",
    "        Crawl NYT to obtain all the information\n",
    "        \"\"\"\n",
    "\n",
    "        page_responses = []\n",
    "        \n",
    "        resp = make_request()\n",
    "        articles = obtain_articles_info(resp)\n",
    "        page_responses.append(articles)\n",
    "        # Get number of articles that match our query search parameters\n",
    "        hits = resp_json[\"response\"][\"meta\"][\"hits\"]\n",
    "\n",
    "        # Get maximum number of pages we can query\n",
    "        max_pages = int(hits / 10)\n",
    "\n",
    "        for page_n in range(1, max_pages + 1):\n",
    "            #NYT has a limit of 5 requests per minute\n",
    "            time.sleep(12)\n",
    "            resp = make_request(page = str(page_n))\n",
    "            status = resp.get(\"status\")\n",
    "            if status== \"OK\":\n",
    "                articles = obtain_articles_info(resp)\n",
    "                page_responses.append(articles)\n",
    "            else:\n",
    "                print(resp)\n",
    "                break\n",
    "        return page_responses\n",
    "\n",
    "test = crawl(begin_date=\"20230101\", end_date=\"20230131\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
