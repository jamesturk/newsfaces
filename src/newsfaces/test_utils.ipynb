{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import crawl_wayback, create_csv, get_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=crawl_wayback(\"https://www.washingtontimes.com/news/politics/?page=1\", 30, get_urls, '20221228', ['article'])\n",
    "create_csv(articles, 'testing', \"test.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-01 00:00:00+00:00 next 2022-01-10 00:00:00+00:00\n",
      "https://web.archive.org/web/20220101193720/https://thehill.com/policy\n",
      "Fetching https://web.archive.org/web/20220101193720/https://thehill.com/policy\n",
      "2022-01-04 01:19:52+00:00 next 2022-01-10 00:00:00+00:00\n",
      "https://web.archive.org/web/20220104011952/https://thehill.com/policy\n",
      "Fetching https://web.archive.org/web/20220104011952/https://thehill.com/policy\n",
      "2022-01-04 18:42:07+00:00 next 2022-01-10 00:00:00+00:00\n",
      "https://web.archive.org/web/20220104184207/https://thehill.com/policy\n",
      "Fetching https://web.archive.org/web/20220104184207/https://thehill.com/policy\n",
      "2022-01-05 01:09:18+00:00 next 2022-01-10 00:00:00+00:00\n",
      "https://web.archive.org/web/20220105010918/https://thehill.com/policy\n",
      "Fetching https://web.archive.org/web/20220105010918/https://thehill.com/policy\n",
      "2022-01-08 22:47:23+00:00 next 2022-01-10 00:00:00+00:00\n",
      "https://web.archive.org/web/20220108224723/https://thehill.com/policy\n",
      "Fetching https://web.archive.org/web/20220108224723/https://thehill.com/policy\n",
      "2022-01-09 10:41:36+00:00 next 2022-01-10 00:00:00+00:00\n",
      "https://web.archive.org/web/20220109104136/https://thehill.com/policy\n",
      "Fetching https://web.archive.org/web/20220109104136/https://thehill.com/policy\n",
      "2022-01-09 23:33:16+00:00 next 2022-01-10 00:00:00+00:00\n",
      "https://web.archive.org/web/20220109233316/https://thehill.com/policy\n",
      "Fetching https://web.archive.org/web/20220109233316/https://thehill.com/policy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Util Functions\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import lxml.html\n",
    "from wayback import WaybackClient, memento_url_data, WaybackSession\n",
    "import itertools\n",
    "import datetime\n",
    "from utils import make_request, parse_html, make_link_absolute, page_grab \n",
    "from crawler import Fox, NBC,WashingtonTimes, TheHill, AP\n",
    "    \n",
    "a=TheHill()\n",
    "\n",
    "#a=AP()\n",
    "a.crawl([2022,1,1], [2022,1,10],4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Washington"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util Functions\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import lxml.html\n",
    "from wayback import WaybackClient, memento_url_data, WaybackSession\n",
    "import itertools\n",
    "import datetime\n",
    "from utils import make_request, parse_html, make_link_absolute, page_grab \n",
    "\n",
    "DEFAULT_DELAY = 0.5\n",
    "\n",
    "\n",
    "class Crawler(url, selectors):\n",
    "    \"\"\"\n",
    "    Need to define at least two properties:\n",
    "    * start_url: the URL to start crawling from\n",
    "    * selectors: a list of css selectors\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url, selectors):\n",
    "        self.session = requests.Session()\n",
    "        self.delay = DEFAULT_DELAY\n",
    "        self.url=url\n",
    "        self.selector=selectors\n",
    "\n",
    "    def make_request(self, url):\n",
    "        \"\"\"\n",
    "        Make a request to `url` and return the raw response.\n",
    "\n",
    "        This function ensure that the domain matches what is expected and that the rate limit\n",
    "        is obeyed.\n",
    "        \"\"\"\n",
    "        # check if URL starts with an allowed domain name\n",
    "        time.sleep(self.delay)\n",
    "        print(f\"Fetching {url}\")\n",
    "        resp = self.session.get(url)\n",
    "        return lxml.html.fromstring(resp.text)\n",
    "\n",
    "    def crawl(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Crawl the site and return a list of URLs to be scraped.\n",
    "        \"\"\"\n",
    "        return self.get_urls(self.start_url, self.selectors)\n",
    "\n",
    "    def get_urls(self, url, selectors):\n",
    "        \"\"\"\n",
    "        This function takes a URLs and returns lists of URLs\n",
    "        for containing each article on that page.\n",
    "\n",
    "        Parameters:\n",
    "            * url:  a URL to a page of articles\n",
    "            * selectors: a list of css selectors\n",
    "\n",
    "        Returns:\n",
    "            A list of article URLs on that page.\n",
    "        \"\"\"\n",
    "        response = self.make_request(url)\n",
    "        urls = []\n",
    "        for selector in selectors:\n",
    "            container = response.cssselect(selector)\n",
    "            for j in container:\n",
    "                atr = j.cssselect(\"a\")\n",
    "                if atr and len(atr) > 0:\n",
    "                    href = atr[0].get(\"href\")\n",
    "                    if len(href) > 0:\n",
    "                        urls.append(\n",
    "                            make_link_absolute(href, \"https://web.archive.org/\")\n",
    "                        )\n",
    "        return urls\n",
    "\n",
    "\n",
    "class WaybackCrawler(Crawler):\n",
    "    def __init__(self):\n",
    "        self.session = WaybackSession()\n",
    "        self.client = WaybackClient(self.session)\n",
    "\n",
    "    # def crawl(self, startdate, break_point):\n",
    "    #     results = self.client.search(self.url, match_type=\"exact\", from_date=startdate)\n",
    "    #     crosstime_urls = list(itertools.islice(results, break_point))\n",
    "    #     post_date_articles = set()\n",
    "    #     for i in range(len(crosstime_urls)):\n",
    "    #         date = datetime.datetime.strptime(startdate, \"%Y%m%d\")\n",
    "    #         if crosstime_urls[i].timestamp.date() >= date.date():\n",
    "    #             articles = self.get_archive_urls(crosstime_urls[i].view_url, Crawler.selectors)\n",
    "    #             # converts archive links back to current article links\n",
    "    #             articles = [memento_url_data(item)[0] for item in articles]\n",
    "    #             post_date_articles.update(articles)\n",
    "    #     return post_date_articles\n",
    "\n",
    "    def crawl(self,startdate,enddate,delta_hrs):\n",
    "        #Create datetime - objects to crawl using wayback\n",
    "        year, month, day = startdate\n",
    "        current_date = datetime.datetime(year,month,day)\n",
    "        year, month, day = enddate\n",
    "        end_date = datetime.datetime(year,month,day)\n",
    "        post_date_articles = set()\n",
    "\n",
    "        last_url_visited = None\n",
    "\n",
    "        #Crawl internet archive once every delta_hrs from startdate until enddate\n",
    "        while current_date != end_date:\n",
    "            results = self.client.search(self.url, match_type=\"exact\", from_date=current_date)\n",
    "            record = next(results)\n",
    "            url = record.view_url\n",
    "            #To avoid fetching urls multiple times, check if there are no updates in\n",
    "            #the delta_hrs period\n",
    "            if last_url_visited != url:\n",
    "                articles = self.get_archive_urls(url,self.selector,self.session)\n",
    "                articles = [memento_url_data(item)[0] for item in articles]\n",
    "                post_date_articles.update(articles)\n",
    "\n",
    "            last_url_visited = url\n",
    "            current_date += datetime.timedelta(hours = delta_hrs)\n",
    "        return post_date_articles\n",
    "    \n",
    "    def get_archive_urls(self, url, selectors):\n",
    "        \"\"\"\n",
    "        might be overriden in child class\n",
    "        \"\"\"\n",
    "        return self.get_urls(url, selectors)\n",
    "\n",
    "\n",
    "class DailyCaller(Crawler):\n",
    "    def crawl(self):\n",
    "        \"\"\"\n",
    "        Implement crawl here to override behavior\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class WashingtonPost(WaybackCrawler):\n",
    "    def get_archive_urls(self, url, selectors):\n",
    "        \"\"\"\n",
    "        Implement get_archive_urls here to override behavior\n",
    "        \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
