{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://www.nbcnews.com/politics/\n"
     ]
    }
   ],
   "source": [
    "import Scraper_Class_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Scraper_Class_obj.BBC at 0x11535db40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Scraper_Class_obj.Fox()\n",
    "b = Scraper_Class_obj.Fox_API()\n",
    "c = Scraper_Class_obj.WashingtonTimes()\n",
    "d = Scraper_Class_obj.NBC()\n",
    "e = Scraper_Class_obj.Politico()\n",
    "f = Scraper_Class_obj.TheHill()\n",
    "g = Scraper_Class_obj.AP()\n",
    "h = Scraper_Class_obj.BBC_Latest()\n",
    "i = Scraper_Class_obj.BBC()\n",
    "j = Scraper_Class_obj.WashingtonPost_API()\n",
    "k = Scraper_Class_obj.WashingtonPost()\n",
    "g\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023, 2, 3]\n",
      "2023-02-03 00:00:00+00:00 next 2023-04-03 00:00:00+00:00\n",
      "Fetching https://web.archive.org/web/20230206153817/https://www.bbc.com/news/topics/cwnpxwzd269t\n",
      "{':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64407273', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-12221823', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64388523', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64460736', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64347994', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64362655', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64370167', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64488011', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64293743', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64404496', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64442720', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64275373', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64489485', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64267983', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64392871', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64290971', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64270906', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64355407', ':///web/20230206153817/https://www.bbc.com/news/business-64322574', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64493028', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64341875', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64362358', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64321153', ':///web/20230206153817/https://www.bbc.com/news/world-us-canada-64347161'}\n",
      "24\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\":///web/20230206153817/https://www.bbc.com/news/world-us-canada-64347161\" is not a memento URL",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m i\u001b[39m.\u001b[39;49mcrawl([\u001b[39m2023\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m],[\u001b[39m2023\u001b[39;49m,\u001b[39m4\u001b[39;49m,\u001b[39m3\u001b[39;49m])\n",
      "File \u001b[0;32m~/newsfaces/src/newsfaces/crawlers/crawler.py:103\u001b[0m, in \u001b[0;36mWaybackCrawler.crawl\u001b[0;34m(self, startdate, enddate, delta_hrs)\u001b[0m\n\u001b[1;32m    101\u001b[0m articles \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_archive_urls(waybackurl, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselector)\n\u001b[1;32m    102\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(articles))\n\u001b[0;32m--> 103\u001b[0m articles \u001b[39m=\u001b[39m  [memento_url_data(item)[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m (item\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39m/web/\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m item\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mweb.archive.org\u001b[39m\u001b[39m'\u001b[39m) \u001b[39min\u001b[39;00m item) \u001b[39melse\u001b[39;00m item \n\u001b[1;32m    104\u001b[0m              \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m articles]\n\u001b[1;32m    105\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(articles))\n\u001b[1;32m    106\u001b[0m post_date_articles\u001b[39m.\u001b[39mupdate(articles)\n",
      "File \u001b[0;32m~/newsfaces/src/newsfaces/crawlers/crawler.py:103\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    101\u001b[0m articles \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_archive_urls(waybackurl, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselector)\n\u001b[1;32m    102\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(articles))\n\u001b[0;32m--> 103\u001b[0m articles \u001b[39m=\u001b[39m  [memento_url_data(item)[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m (item\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39m/web/\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m item\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mweb.archive.org\u001b[39m\u001b[39m'\u001b[39m) \u001b[39min\u001b[39;00m item) \u001b[39melse\u001b[39;00m item \n\u001b[1;32m    104\u001b[0m              \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m articles]\n\u001b[1;32m    105\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(articles))\n\u001b[1;32m    106\u001b[0m post_date_articles\u001b[39m.\u001b[39mupdate(articles)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/wayback/_utils.py:156\u001b[0m, in \u001b[0;36mmemento_url_data\u001b[0;34m(memento_url)\u001b[0m\n\u001b[1;32m    154\u001b[0m match \u001b[39m=\u001b[39m MEMENTO_URL_PATTERN\u001b[39m.\u001b[39mmatch(memento_url)\n\u001b[1;32m    155\u001b[0m \u001b[39mif\u001b[39;00m match \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmemento_url\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is not a memento URL\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    158\u001b[0m url \u001b[39m=\u001b[39m clean_memento_url_component(match\u001b[39m.\u001b[39mgroup(\u001b[39m3\u001b[39m))\n\u001b[1;32m    159\u001b[0m date \u001b[39m=\u001b[39m parse_timestamp(match\u001b[39m.\u001b[39mgroup(\u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: \":///web/20230206153817/https://www.bbc.com/news/world-us-canada-64347161\" is not a memento URL"
     ]
    }
   ],
   "source": [
    "i.crawl([2023, 2, 3], [2023, 4, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crawl_wayback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m articles\u001b[39m=\u001b[39mcrawl_wayback(\u001b[39m\"\u001b[39m\u001b[39mhttps://www.washingtontimes.com/news/politics/?page=1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m30\u001b[39m, get_urls, \u001b[39m'\u001b[39m\u001b[39m20221228\u001b[39m\u001b[39m'\u001b[39m, [\u001b[39m'\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m create_csv(articles, \u001b[39m'\u001b[39m\u001b[39mtesting\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtest.csv\u001b[39m\u001b[39m\"\u001b[39m,)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crawl_wayback' is not defined"
     ]
    }
   ],
   "source": [
    "articles = crawl_wayback(\n",
    "    \"https://www.washingtontimes.com/news/politics/?page=1\",\n",
    "    30,\n",
    "    get_urls,\n",
    "    \"20221228\",\n",
    "    [\"article\"],\n",
    ")\n",
    "create_csv(\n",
    "    articles,\n",
    "    \"testing\",\n",
    "    \"test.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util Functions\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import lxml.html\n",
    "from wayback import WaybackClient, memento_url_data, WaybackSession\n",
    "import itertools\n",
    "import datetime\n",
    "from utils import make_request, parse_html, make_link_absolute, page_grab\n",
    "from crawler import Fox, NBC, WashingtonTimes, TheHill, AP\n",
    "\n",
    "\n",
    "a = Fox()\n",
    "a.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = WashingtonPost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util Functions\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import lxml.html\n",
    "from wayback import WaybackClient, memento_url_data, WaybackSession\n",
    "import itertools\n",
    "import datetime\n",
    "from utils import make_request, parse_html, make_link_absolute, page_grab\n",
    "\n",
    "DEFAULT_DELAY = 0.5\n",
    "\n",
    "\n",
    "class Crawler(url, selectors):\n",
    "    \"\"\"\n",
    "    Need to define at least two properties:\n",
    "    * start_url: the URL to start crawling from\n",
    "    * selectors: a list of css selectors\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url, selectors):\n",
    "        self.session = requests.Session()\n",
    "        self.delay = DEFAULT_DELAY\n",
    "        self.url = url\n",
    "        self.selector = selectors\n",
    "\n",
    "    def make_request(self, url):\n",
    "        \"\"\"\n",
    "        Make a request to `url` and return the raw response.\n",
    "\n",
    "        This function ensure that the domain matches what is expected and that the rate limit\n",
    "        is obeyed.\n",
    "        \"\"\"\n",
    "        # check if URL starts with an allowed domain name\n",
    "        time.sleep(self.delay)\n",
    "        print(f\"Fetching {url}\")\n",
    "        resp = self.session.get(url)\n",
    "        return lxml.html.fromstring(resp.text)\n",
    "\n",
    "    def crawl(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Crawl the site and return a list of URLs to be scraped.\n",
    "        \"\"\"\n",
    "        return self.get_urls(self.start_url, self.selectors)\n",
    "\n",
    "    def get_urls(self, url, selectors):\n",
    "        \"\"\"\n",
    "        This function takes a URLs and returns lists of URLs\n",
    "        for containing each article on that page.\n",
    "\n",
    "        Parameters:\n",
    "            * url:  a URL to a page of articles\n",
    "            * selectors: a list of css selectors\n",
    "\n",
    "        Returns:\n",
    "            A list of article URLs on that page.\n",
    "        \"\"\"\n",
    "        response = self.make_request(url)\n",
    "        urls = []\n",
    "        for selector in selectors:\n",
    "            container = response.cssselect(selector)\n",
    "            for j in container:\n",
    "                atr = j.cssselect(\"a\")\n",
    "                if atr and len(atr) > 0:\n",
    "                    href = atr[0].get(\"href\")\n",
    "                    if len(href) > 0:\n",
    "                        urls.append(\n",
    "                            make_link_absolute(href, \"https://web.archive.org/\")\n",
    "                        )\n",
    "        return urls\n",
    "\n",
    "\n",
    "class WaybackCrawler(Crawler):\n",
    "    def __init__(self):\n",
    "        self.session = WaybackSession()\n",
    "        self.client = WaybackClient(self.session)\n",
    "\n",
    "    # def crawl(self, startdate, break_point):\n",
    "    #     results = self.client.search(self.url, match_type=\"exact\", from_date=startdate)\n",
    "    #     crosstime_urls = list(itertools.islice(results, break_point))\n",
    "    #     post_date_articles = set()\n",
    "    #     for i in range(len(crosstime_urls)):\n",
    "    #         date = datetime.datetime.strptime(startdate, \"%Y%m%d\")\n",
    "    #         if crosstime_urls[i].timestamp.date() >= date.date():\n",
    "    #             articles = self.get_archive_urls(crosstime_urls[i].view_url, Crawler.selectors)\n",
    "    #             # converts archive links back to current article links\n",
    "    #             articles = [memento_url_data(item)[0] for item in articles]\n",
    "    #             post_date_articles.update(articles)\n",
    "    #     return post_date_articles\n",
    "\n",
    "    def crawl(self, startdate, enddate, delta_hrs):\n",
    "        # Create datetime - objects to crawl using wayback\n",
    "        year, month, day = startdate\n",
    "        current_date = datetime.datetime(year, month, day)\n",
    "        year, month, day = enddate\n",
    "        end_date = datetime.datetime(year, month, day)\n",
    "        post_date_articles = set()\n",
    "\n",
    "        last_url_visited = None\n",
    "\n",
    "        # Crawl internet archive once every delta_hrs from startdate until enddate\n",
    "        while current_date != end_date:\n",
    "            results = self.client.search(\n",
    "                self.url, match_type=\"exact\", from_date=current_date\n",
    "            )\n",
    "            record = next(results)\n",
    "            url = record.view_url\n",
    "            # To avoid fetching urls multiple times, check if there are no updates in\n",
    "            # the delta_hrs period\n",
    "            if last_url_visited != url:\n",
    "                articles = self.get_archive_urls(url, self.selector, self.session)\n",
    "                articles = [memento_url_data(item)[0] for item in articles]\n",
    "                post_date_articles.update(articles)\n",
    "\n",
    "            last_url_visited = url\n",
    "            current_date += datetime.timedelta(hours=delta_hrs)\n",
    "        return post_date_articles\n",
    "\n",
    "    def get_archive_urls(self, url, selectors):\n",
    "        \"\"\"\n",
    "        might be overriden in child class\n",
    "        \"\"\"\n",
    "        return self.get_urls(url, selectors)\n",
    "\n",
    "\n",
    "class DailyCaller(Crawler):\n",
    "    def crawl(self):\n",
    "        \"\"\"\n",
    "        Implement crawl here to override behavior\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class WashingtonPost(WaybackCrawler):\n",
    "    def get_archive_urls(self, url, selectors):\n",
    "        \"\"\"\n",
    "        Implement get_archive_urls here to override behavior\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
