{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://www.politico.com/news/2023/08/07/bidenomics-white-house-economy-00109977\n",
      "[] vidoes\n",
      "[] captions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Article(title='\\nThe White House plays it cool as ‘Bidenomics’ struggles to catch on\\n', article_text='\\nDemocrats acknowledge that slapping Joe Biden’s name on the economy is a gamble, given the prospect of it moving in the wrong direction. | Susan Walsh/AP Photo\\nBy Jennifer Haberkorn\\n08/07/2023 05:01 AM EDT\\nLink CopiedPresident Joe Biden is risking a lot on “Bidenomics.” But, about two months in, his efforts to sell his sweeping economic agenda don’t appear to be working.Poll numbers show persistent voter skepticism about the state of the economy, and Republicans are working aggressively to take back the term, dubbing it as synonymous with tax hikes and inflation.Inside the White House, aides remain confident the bet will pay off, adopting the mantra of the hockey legend Wayne Gretzky: Skate to where the puck is going, not where it is now.', images=[Image(url='https://www.politico.com/dims4/default/e8e9ac9/2147483647/strip/true/crop/5837x3891+0+0/resize/630x420!/quality/90/?url=https%3A%2F%2Fstatic.politico.com%2F40%2F67%2F12d5292341bda347feb706533797%2Fbiden-14579.jpg', image_type=<ImageType.main: 'main'>, caption='', alt_text='President Joe Biden speaks at Auburn Manufacturing Inc.'), Image(url='https://static.politico.com/40/67/12d5292341bda347feb706533797/biden-14579.jpg', image_type=<ImageType.social: 'social'>, caption='', alt_text=''), Image(url='https://www.politico.com/dims4/default/1445794/2147483647/strip/true/crop/1280x720+0+0/resize/1160x653!/quality/90/?url=https%3A%2F%2Fcf-images.us-east-1.prod.boltdns.net%2Fv1%2Fstatic%2F1155968404%2Fa9ba7341-614d-4b84-b366-b408a3b75c99%2F9e79f2b6-b06c-4475-8c05-2fc7b0cc3280%2F1280x720%2Fmatch%2Fimage.jpg', image_type=<ImageType.video_thumbnail: 'video_thumbnail'>, caption='', alt_text=''), Image(url='https://www.politico.com/dims4/default/4fd9cfd/2147483647/strip/true/crop/1280x720+0+0/resize/1160x653!/quality/90/?url=https%3A%2F%2Fcf-images.us-east-1.prod.boltdns.net%2Fv1%2Fstatic%2F1155968404%2Fbe6c87aa-03ad-4a44-aa78-464c3884b4c3%2F00c44579-82e0-4c9e-b7bb-0f235dc90f17%2F1280x720%2Fmatch%2Fimage.jpg', image_type=<ImageType.video_thumbnail: 'video_thumbnail'>, caption='', alt_text='')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from newsfaces.extract_html import Extractor\n",
    "from newsfaces.utils import make_link_absolute, page_grab, make_request\n",
    "from newsfaces.crawlers.crawler import Crawler\n",
    "from newsfaces.models import Image, Article, ImageType\n",
    "\n",
    "\n",
    "class Politico(Crawler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"\n",
    "        Implement crawl here to override behavior\n",
    "        \"\"\"\n",
    "        return self.politico_get_urls()\n",
    "\n",
    "    def get_urls(self, url):\n",
    "        \"\"\"\n",
    "        This function takes a URLs and returns lists of URLs\n",
    "        for containing each article on that page.\n",
    "\n",
    "        Parameters:\n",
    "            * url:  a URL to a page of articles\n",
    "\n",
    "        Returns:\n",
    "            A list of article URLs on that page.\n",
    "        \"\"\"\n",
    "        response = self.make_request(url)\n",
    "        urls = []\n",
    "        container = response.cssselect(\"div.summary\")\n",
    "\n",
    "        for j in container:\n",
    "            atr = j.cssselect(\"a\")\n",
    "            if atr and len(atr) > 0:\n",
    "                href = atr[0].get(\"href\")\n",
    "                urls.append(make_link_absolute(href, \"https://www.politico.com\"))\n",
    "        return urls\n",
    "\n",
    "    def politico_get_urls(self):\n",
    "        urls=set()\n",
    "        for page in range(1, 3400):\n",
    "            urls = urls.union(self.get_urls(f\"https://www.politico.com/politics/{page}\"))\n",
    "        return urls\n",
    "\n",
    "class Politico_Extractor(Extractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.article_body = [\"div.story-text\"]\n",
    "        self.img_p_selector = [\n",
    "            \"section.media-item.media-item--story.media-item--story-lead\"\n",
    "        ]\n",
    "        self.img_selector = [\"img\"]\n",
    "        self.head_img_div = [\n",
    "            \"section.media-item.media-item--story.media-item--story-lead\"\n",
    "        ]\n",
    "        self.video = [\"div.media-item__video\"]\n",
    "        self.head_img_select = [\"img\"]\n",
    "        self.p_selector = [\"p\"]\n",
    "        self.t_selector = [\"h2.headline\"]\n",
    "\n",
    "    def scrape(self, url):\n",
    "        \"\"\"\n",
    "        Extract html and from\n",
    "        \"\"\"\n",
    "        html = page_grab(url)\n",
    "        imgs, art_text, t_text = self.extract_html(html)\n",
    "        imgs += self.extract_video_imgs(html)\n",
    "        article = Article(title=t_text or \"\", article_text=art_text or \"\", images=imgs)\n",
    "        return article\n",
    "\n",
    "    def extract_video_imgs(self, html):\n",
    "        videos = []\n",
    "        imgs = []\n",
    "\n",
    "        for i in self.video:\n",
    "            videos += html.cssselect(i)\n",
    "        item = []\n",
    "        cap_elements = []\n",
    "        for v in videos:\n",
    "            item += v.cssselect(\"video\")\n",
    "        cap_elements += html.xpath('//div[contains(@class, \"vjs-dock-title\")]')\n",
    "        print(cap_elements, \"vidoes\")\n",
    "        # Extract captions from cap_elements\n",
    "        captions = [element.text_content() for element in cap_elements]\n",
    "        print(cap_elements, \"captions\")\n",
    "\n",
    "        for i, video in enumerate(item):\n",
    "            \n",
    "            img_item = Image(\n",
    "                url=video.get(\"poster\") or \"\",\n",
    "                image_type=ImageType(\"video_thumbnail\"),\n",
    "                caption=captions[i] if i < len(captions) else \"\",\n",
    "                alt_text=\"\",\n",
    "            )\n",
    "            imgs.append(img_item)\n",
    "        return imgs\n",
    "      \n",
    "a=Politico_Extractor()\n",
    "a.scrape('https://www.politico.com/news/2023/08/07/bidenomics-white-house-economy-00109977')\n",
    "#b=Politico()\n",
    "#b.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://apnews.com/article/-----804f40cacfa94097a0e5311d1604afec\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 96\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_selector \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mh1.Page-headline\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     95\u001b[0m a\u001b[39m=\u001b[39m AP_Extractor()\n\u001b[0;32m---> 96\u001b[0m a\u001b[39m.\u001b[39;49mscrape(\u001b[39m'\u001b[39;49m\u001b[39mhttps://apnews.com/article/-----804f40cacfa94097a0e5311d1604afec\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     98\u001b[0m \u001b[39m#html=page_grab('https://apnews.com/article/nepal-rice-day-festival-harvest-farmers-ae3cd725bd1784d29ea499c2ba383f21')\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m#print(lxml.etree.tostring(html))\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m  \u001b[39m#a= page_grab('https://apnews.com/article/trump-georgia-election-investigation-grand-jury-willis-d39562cedfc60d64948708de1b011ed3')\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m# extract_imgs(a, [\"figure.Figure\"], [\"img\"])\u001b[39;00m\n",
      "File \u001b[0;32m~/newsfaces/src/newsfaces/extract_html.py:147\u001b[0m, in \u001b[0;36mExtractor.scrape\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39mExtract html and from\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m html \u001b[39m=\u001b[39m page_grab(url)\n\u001b[0;32m--> 147\u001b[0m imgs, art_text, t_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_html(html)\n\u001b[1;32m    148\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(imgs))\n\u001b[1;32m    149\u001b[0m article \u001b[39m=\u001b[39m Article(title\u001b[39m=\u001b[39mt_text \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, article_text\u001b[39m=\u001b[39mart_text \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, images\u001b[39m=\u001b[39mimgs)\n",
      "File \u001b[0;32m~/newsfaces/src/newsfaces/extract_html.py:43\u001b[0m, in \u001b[0;36mExtractor.extract_html\u001b[0;34m(self, html)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_img_div:\n\u001b[0;32m---> 43\u001b[0m     imgs \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_head_img(html, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead_img_div, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead_img_select)\n\u001b[1;32m     44\u001b[0m imgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_imgs(article_body, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_p_selector, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_selector)\n\u001b[1;32m     45\u001b[0m imgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_social_media_image(html)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "from newsfaces.utils import make_link_absolute\n",
    "from newsfaces.crawlers.crawler import WaybackCrawler\n",
    "# Util Functions\n",
    "from newsfaces.extract_html import Extractor\n",
    "from newsfaces.utils import make_link_absolute, page_grab\n",
    "from newsfaces.crawlers.crawler import Crawler\n",
    "from newsfaces.models import Image, Article, ImageType\n",
    "import datetime\n",
    "import pytz\n",
    "import lxml\n",
    "\n",
    "\n",
    "class AP(WaybackCrawler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.urls = [\"https://apnews.com/politics\", \"https://apnews.com/hub/politics\"]\n",
    "        self.start_url = \"\"\n",
    "        self.selector = []\n",
    "\n",
    "    def get_archive_urls(self, url, selectors):\n",
    "        \"\"\"\n",
    "        This function takes a URLs and returns lists of URLs\n",
    "        for containing each article on that page.\n",
    "\n",
    "        Parameters:\n",
    "            * url:  a URL to a page of articles\n",
    "            * session: optional session object parameter\n",
    "            * selectors: a list of css selectors\n",
    "\n",
    "        Returns:\n",
    "            A list of article URLs on that page.\n",
    "        \"\"\"\n",
    "        response = self.make_request(url)\n",
    "        urls = []\n",
    "        selectors = [\n",
    "            \"div.FourColumnContainer-column\",\n",
    "            \"div.TwoColumnContainer7030\",\n",
    "            \"div.PageList-items\",\n",
    "            \"article\",\n",
    "        ]\n",
    "        for a in selectors:\n",
    "            container = response.cssselect(a)\n",
    "            if len(container) > 0:\n",
    "                urls += self.parse_links(container)\n",
    "        xpath_sel = [\"TwoColumnContainer\", \"CardHeadline\"]\n",
    "        # for items that have random characters continually added at the end so we do non-exact matching\n",
    "        for j in xpath_sel:\n",
    "            container = response.xpath(f\"//div[contains(@class, '{j}')]\")\n",
    "            if len(container) > 0:\n",
    "                urls += self.parse_links(container)\n",
    "\n",
    "        return urls\n",
    "\n",
    "    def crawl(self, startdate, enddate, delta_hrs=6):\n",
    "        if startdate < datetime.datetime(\n",
    "            2023, 6, 26, 0, 0, tzinfo=pytz.timezone(\"utc\")\n",
    "        ):\n",
    "            self.start_url = self.urls[1]\n",
    "            changepage_date = datetime.datetime(\n",
    "                2023, 6, 26, 0, 0, tzinfo=pytz.timezone(\"utc\")\n",
    "            )\n",
    "            hub_site = super().crawl(startdate, changepage_date, delta_hrs)\n",
    "        self.start_url = self.urls[0]\n",
    "        current_site = super().crawl(startdate, enddate, delta_hrs)\n",
    "        return hub_site.union(current_site)\n",
    "\n",
    "    def parse_links(self, container):\n",
    "        \"\"\"\n",
    "        Takes a list of container objects and returns the urls\n",
    "        from within\n",
    "        \"\"\"\n",
    "        urls = []\n",
    "        for j in container[0]:\n",
    "            atr = j.cssselect(\"a\")\n",
    "            for a in atr:\n",
    "                href = a.get(\"href\")\n",
    "                if href is not None:\n",
    "                    if href.startswith(\"/web/\"):\n",
    "                        href = make_link_absolute(href, \"https://web.archive.org\")\n",
    "                    urls.append(href)\n",
    "        return urls\n",
    "\n",
    "\n",
    "class AP_Extractor(Extractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.article_body = [\"main.Page-main\"]\n",
    "        self.img_p_selector = [\"figure.Figure\"] \n",
    "        self.img_selector = [\"img\"]\n",
    "        self.head_img_div = [\"div.Page-lead\"]\n",
    "        self.head_img_select = [\"img\"]\n",
    "        self.p_selector = [\"p\"]\n",
    "        self.t_selector = [\"h1.Page-headline\"]\n",
    "\n",
    "a= AP_Extractor()\n",
    "a.scrape('https://apnews.com/article/-----804f40cacfa94097a0e5311d1604afec')\n",
    "\n",
    "#html=page_grab('https://apnews.com/article/nepal-rice-day-festival-harvest-farmers-ae3cd725bd1784d29ea499c2ba383f21')\n",
    "#print(lxml.etree.tostring(html))\n",
    "\n",
    "# def extract_imgs(html, img_p_selector, img_selector):\n",
    "#     \"\"\"\n",
    "#     Extract the image content from an HTML:\n",
    "#     Inputs:\n",
    "#         - html(str): html to extract images from\n",
    "#         - img_p_selector(list): css selector for the parent elements of images\n",
    "#         - img_selector(list): css selector for the image elements\n",
    "#         Return:\n",
    "#         -imgs(lst): each element is an image represented as an image object\n",
    "#     \"\"\"\n",
    "#     imgs = []\n",
    "#     for selector in img_p_selector:\n",
    "#         img_container = html.cssselect(selector)\n",
    "#         for container in img_container:\n",
    "#             for j in img_selector:\n",
    "#                 photos = container.cssselect(j)\n",
    "#                 for i in photos:\n",
    "#                     img_item = Image(\n",
    "#                         url=i.get(\"src\") or \"\",\n",
    "#                         image_type=ImageType(\"main\"),\n",
    "#                         caption=i.get(\"caption\") or \"\",\n",
    "#                         alt_text=i.get(\"alt\") or \"\",\n",
    "#                     )\n",
    "#                 imgs.append(img_item)\n",
    "#     return imgs\n",
    "\n",
    " #a= page_grab('https://apnews.com/article/trump-georgia-election-investigation-grand-jury-willis-d39562cedfc60d64948708de1b011ed3')\n",
    "# extract_imgs(a, [\"figure.Figure\"], [\"img\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://www.bbc.com/news/world-us-canada-65394456\n",
      "figure\n",
      "[<Element figure at 0x10e8810e0>, <Element figure at 0x108aedae0>]\n",
      "[class^=\"mediaContainer\"]\n",
      "[]\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Article(title='Biden aide gaffe leads to campaign clarification', article_text='White House Press Secretary Karine Jean-Pierre has clarified President Joe Biden\\'s re-election plans after a comment she made sparked confusion. Hours after Mr Biden formally launched his campaign for a second four-year term in office she refused to say if he planned to complete the entire term. \"That\\'s something for him to decide,\" she said at a briefing. Mr Biden, 80, is the oldest president in US history. She later clarified that he would serve another full term in office if he wins. The spokeswoman\\'s embarrassing about-face came only hours after Mr Biden officially announced he was running again in a pre-recorded video.\"Does the president plan to serve all eight years?\" a reporter for Politico asked at a White House briefing on Tuesday. \"That\\'s something for him to decide,\" she began.\"I\\'m not just not going to get ahead of it. And there\\'s a 2024 campaign. Anything related to that, I would refer you to that.\"Ms Jean-Pierre said she had initially avoided answering the reporter\\'s question to remain compliant with a US federal election law known as the Hatch Act which prohibits executive branch employees from using their position to campaign for a candidate while performing official duties. According to Politico, Ms Jean-Pierre is known to often refer reporters to the Hatch Act. Reporters complain that she has interpreted the law too broadly and cites it to avoid answering certain questions.On Tuesday she cited the law again when asked about the media contact for Mr Biden\\'s re-election campaign and whether the war in Ukraine had impacted his decision to run. Later in the day she took to Twitter to pledge that Mr Biden would serve all eight years if he won re-election. \"As you know, we take following the law seriously,\" she posted.  \"So I wanted to be sure that I didn\\'t go into 2024 more than is appropriate under the law. But I can confirm that if re-elected, @POTUS would serve all 8 years.\"Kellyanne Conway, a longtime campaign and White House aide to former president Donald Trump, was found by a US government watchdog to have repeatedly violated the Hatch Act. The White House Office of Special Counsel wrote that her failure to comply with the law wil \"erode the principal foundation of our democratic system - the rule of law\" and called for her to be fired. Ms Jean-Pierre\\'s gaffe comes amid new polling that suggests a majority of Americans think President Biden should not run for re-election because of his age. Mr Biden, 80, who is already the oldest serving US president, would be 86 at the end of his second term. This video can not be playedWatch: Older Americans weigh in on President Biden\\'s ageBiden launches 2024 re-election campaignWhite House names first black press secretaryBiden v Trump: The sequel few Americans want to see', images=[Image(url='https://ichef.bbci.co.uk/news/976/cpsprodpb/35F2/production/_129501831_karine.jpg', image_type=<ImageType.main: 'main'>, caption='', alt_text='Karine Jean-Pierre'), Image(url='https://ichef.bbci.co.uk/news/1024/branded_news/5D02/production/_129501832_karine.jpg', image_type=<ImageType.social: 'social'>, caption='', alt_text='')])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from newsfaces.crawlers.crawler import Crawler, WaybackCrawler\n",
    "from newsfaces.utils import make_link_absolute\n",
    "\n",
    "\n",
    "class BBC_Latest(Crawler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.start_url = \"https://www.bbc.com/news/topics/cwnpxwzd269t?page=1\"\n",
    "    \n",
    "    def crawl(self):\n",
    "        '''\n",
    "        run get_html with correct initial html from init\n",
    "        '''\n",
    "        return self.get_newslink(self.start_url)\n",
    "    \n",
    "    def get_newslink(self, url, articles=set(), videos=set()):\n",
    "        \"\"\"\n",
    "        Takes an initial url and runs get_urls on all possible\n",
    "        API queries. Gathering all possible articles and videos\n",
    "        from the API into a set.\n",
    "        \"\"\"\n",
    "        article, video = self.get_urls(url)\n",
    "        articles = articles.union(article)\n",
    "        videos = videos.union(video)\n",
    "        begin = url.find(\"page=\") + 5\n",
    "        pagenumber = int(url[begin : len(url)])\n",
    "        if pagenumber < 42:\n",
    "            newlink = url[: -len(str(pagenumber))] + str(pagenumber + 1)\n",
    "            article, video = self.crawl(newlink, articles, videos)\n",
    "            articles = articles.union(article)\n",
    "            videos = videos.union(video)\n",
    "        return articles, videos\n",
    "\n",
    "    def get_urls(self, url, articles=set(), videos=set()):\n",
    "        \"\"\"\n",
    "        This function takes a URLs and returns lists of URLs\n",
    "        for containing each article and video on that page.\n",
    "\n",
    "        Parameters:\n",
    "            * url:  a URL to a page of articles\n",
    "\n",
    "        Returns:\n",
    "            A list of URLs to each video and article on that page.\n",
    "        \"\"\"\n",
    "        response = self.make_request(url)\n",
    "        container = response.cssselect(\"div\")\n",
    "        filtered_container = [\n",
    "            elem for elem in container if elem.get(\"type\") is not None\n",
    "        ]\n",
    "\n",
    "        for j in filtered_container:\n",
    "            # find video/article\n",
    "            type = j.get(\"type\")\n",
    "            # find link\n",
    "            if type == \"article\" or type == \"video\":\n",
    "                a = j[0].cssselect(\"a\")\n",
    "                href = a[0].get(\"href\")\n",
    "                href = make_link_absolute(href, \"https://www.bbc.com\")\n",
    "            if type == \"article\":\n",
    "                articles.add(href)\n",
    "            elif type == \"video\":\n",
    "                videos.add(href)\n",
    "        return articles, videos\n",
    "\n",
    "\n",
    "class BBC(WaybackCrawler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.start_url = \"https://www.bbc.com/news/topics/cwnpxwzd269t\"\n",
    "        self.selector = [\"div.archive__item__content\", \"h2.node__title.node-title\"]\n",
    "\n",
    "    def get_archive_urls(self, url, selector):\n",
    "        return self.get(url)\n",
    "\n",
    "\n",
    "class BBC(WaybackCrawler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.start_url = \"https://www.bbc.com/news/topics/cwnpxwzd269t\"\n",
    "        self.selector = [\"div.archive__item__content\", \"h2.node__title.node-title\"]\n",
    "\n",
    "    def get_archive_urls(self, url, selector):\n",
    "        return self.get(url)\n",
    "\n",
    "    def get(self, url, articles=set(), videos=set()):\n",
    "        \"\"\"\n",
    "        This function takes a URLs and returns lists of URLs\n",
    "        for containing each article and video on that page.\n",
    "\n",
    "        Parameters:\n",
    "            * url:  a URL to a page of articles\n",
    "\n",
    "        Returns:\n",
    "            A list of URLs to each video and article on that page.\n",
    "        \"\"\"\n",
    "        response = self.make_request(url)\n",
    "        xpath_sel= ['article','video']\n",
    "            # for items that have random characters continually added at the end so we do non-exact matching\n",
    "        for j in xpath_sel:\n",
    "            container = response.xpath(f\"//div[contains(@type, '{j}')]\")\n",
    "            if container:\n",
    "                for j in container:\n",
    "                    a = j[0].cssselect(\"a\")\n",
    "                    href = a[0].get(\"href\")\n",
    "                    href = make_link_absolute(href, \"https://web.archive.org\")\n",
    "                    if j == \"article\":\n",
    "                        articles.add(href)\n",
    "                    else:\n",
    "                        videos.add(href)\n",
    "        return articles.union(videos)\n",
    "    \n",
    "class BBC_Extractor(Extractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.article_body = [\"main\"]\n",
    "        self.img_p_selector = [\"figure\", \"div#mediaContainer\"] \n",
    "        self.img_selector = [\"img\"]\n",
    "        self.head_img_div = []\n",
    "        self.head_img_select = []\n",
    "        self.p_selector = [\"p\"]\n",
    "        self.t_selector = [\"h1\"]\n",
    "\n",
    "    def extract_imgs(self, html, img_p_selector, img_selector):\n",
    "        \"\"\"\n",
    "        Extract the image content from an HTML:\n",
    "        Inputs:\n",
    "            - html(str): html to extract images from\n",
    "            - img_p_selector(list): css selector for the parent elements of images\n",
    "            - img_selector(list): css selector for the image elements\n",
    "            Return:\n",
    "            -imgs(lst): each element is an image represented as an image object\n",
    "        \"\"\"\n",
    "        imgs = []\n",
    "        captions= html.cssselect(\"figcaption\")\n",
    "\n",
    "        for selector in img_p_selector:\n",
    "            img_container = html.cssselect(selector)\n",
    "            print(selector)\n",
    "            print(img_container)\n",
    "            if len(img_container) == 0:\n",
    "                continue\n",
    "            for container in img_container:\n",
    "                for j in img_selector:\n",
    "                    photos = container.cssselect(j)\n",
    "                    for i, photo in enumerate(photos):\n",
    "                        if selector == \"#mediaContainer\":\n",
    "                            type=\"video_thumbnail\"\n",
    "                        else:\n",
    "                            type=\"main\"\n",
    "                        img_item = Image(\n",
    "                            url=photo.get(\"src\") or \"\",\n",
    "                            image_type=ImageType(type),\n",
    "                            caption= captions[i].text or \"\",\n",
    "                            alt_text=photo.get(\"alt\") or \"\",\n",
    "                        )\n",
    "                        imgs.append(img_item)\n",
    "        return imgs\n",
    "    def extract_head_img(self, html, img_p_selector, img_selector):\n",
    "        return []\n",
    "    \n",
    "a= BBC_Extractor()\n",
    "#a.scrape(\"https://www.google.com/amp/s/www.bbc.com/news/world-us-canada-40127326.amp\")\n",
    "a.scrape(\"https://www.bbc.com/news/world-us-canada-65394456\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://www.bbc.com/news/world-us-canada-65394456\n",
      "https://ichef.bbci.co.uk/news/976/cpsprodpb/35F2/production/_129501831_karine.jpg\n",
      "https://ichef.bbci.co.uk/news/976/cpsprodpb/097D/production/_129492420_gettyimages-1252051962-1.jpg\n",
      "https://ichef.bbci.co.uk/news/976/cpsprodpb/3340/production/_124602131_gettyimages-1236348553.jpg\n",
      "https://ichef.bbci.co.uk/news/976/cpsprodpb/102E7/production/_129497266_gettyimages-1228795132.jpg\n",
      "https://ichef.bbci.co.uk/news/385/cpsprodpb/FE23/production/_130795056_afghan_promo.jpg\n",
      "https://ichef.bbci.co.uk/news/385/cpsprodpb/1518/production/_130800450_gettyimages-1248276093.jpg\n",
      "https://ichef.bbci.co.uk/news/385/cpsprodpb/1639F/production/_130793019_bbc-sport-index-imagery-4-split-images-gradient-3a3fde8f-2929-413b-8960-c6c999c356da.png\n",
      "https://ichef.bbci.co.uk/news/385/cpsprodpb/12378/production/_130661647_20230806_105553.jpg\n",
      "https://ichef.bbci.co.uk/news/385/cpsprodpb/D907/production/_130695555_a70c0d19-085e-44a1-8f78-fa9d3d748c2a.jpg\n",
      "https://ichef.bbci.co.uk/news/385/cpsprodpb/7E3F/production/_130791323_tttitle-reuters.jpg\n",
      "https://ichef.bbci.co.uk/news/385/cpsprodpb/1717D/production/_130798549_gettyimages-1591150456.jpg\n",
      "https://ichef.bbci.co.uk/news/385/cpsprodpb/15C0C/production/_130800198_45.jpg\n",
      "https://ichef.bbci.co.uk/news/385/cpsprodpb/13FF/production/_130791150_p0g65lx3.jpg\n",
      "https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/1423A/production/_130709428_thegreatresignationisoversayexperts.jpg\n",
      "https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/D0F2/production/_130709435_theprotectorsofa7000-year-oldfaith.jpg\n",
      "https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/C1B6/production/_130709594_whatcolourshouldyouwearintheheat.jpg\n"
     ]
    }
   ],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "b= page_grab(\"https://www.bbc.com/news/world-us-canada-65394456\")\n",
    "c= b.cssselect(\"img\")\n",
    "for i in c:\n",
    "    print(i.get(\"src\"))\n",
    "    i.tag\n",
    "    parent=i.getparent()\n",
    "    grandparent=parent.getparent()\n",
    "    g=grandparent.getparent()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beakers-wVt3Sn-3-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
